# -*- coding: utf-8 -*-
"""Swiggy Restaurant Recommendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UTgfIUGTqkQxQpwIzk3VNUlWQOdEXpxH

# **04.05.2025**
"""

a=1743
b=a//500
c=a%500
print(b, " x 500 =", b*500)
d=c//200
e=c%200
print(d, " x 200 =", d*200)
f=e//20
g=e%20
print(f, "x 20 =",f*20)
h=g//2
i=g%2
print(h," x 2 =", h*2)
j=i//1
print(j, "x 1 =",j*1)
print("Total = ", a)

a=200
b=300
c=400
d=500
e=a+b+c+d
print(e)

"""# **10.05.2025**"""

a="sethuraman"

a[:7:2]

"""Shepherd, the most dangerous class of T20 finisher
There are other T20 batters who have a better rate of success, whose methods translate over a variety of conditions and oppositions,
but few get on a roll like Romario Shepherd can"""

name="sethuraman"
name[:4]
date=651975
int(date[-4:])

name="sethuraman"
name[7]

name=10.23
print(name,type(name))
type(name)
a=200,type(a)

salary=40000
deduction=1000
net=salary-deduction
print(net)

a="salary",5000,type(a)
b="deduction",500
c="net salary",5000-500
print(c)

data = input()
data_new = int(data)
print(data_new, type(data_new))

data = input()
data_new = int(data) #type casting
print(data_new,type(data_new))

name="sethuraman"
data=651975
data1=123456781278
a = int(data)
b = int(data1)

from PIL import Image

a=Image.open("/content/unnamed.jpg")
a

a=Image.open("/content/unnamed.jpg")
raw=a.load()
width=a.width
height=a.height
for w in range (width):
  for h in range (height):
    r,g,b=raw[w,h]
    r=255-r
    g=255-g
    b=255-b
    raw[w,h]=(r,g,b)

a

#Grey Scale Universal
a=Image.open("/content/unnamed.jpg")
raw=a.load()
width=a.width
height=a.height
for w in range (width):
  for h in range (height):
    r,g,b=raw[w,h]
    r=r*.299
    g=g*.587
    b=b*.114
    s=round(r+g+b)
    raw[w,h]=(s,s,s)

a

!pip install opencv-python
!wget https://raw.githubusercontent.com/richzhang/colorization/master/resources/pts_in_hull.npy
!wget https://raw.githubusercontent.com/richzhang/colorization/master/models/colorization_deploy_v2.prototxt
!wget http://eecs.berkeley.edu/~rich.zhang/projects/2016_colorization/files/demo_v2/colorization_release_v2.caffemodel

# Install OpenCV
!pip install opencv-python

# Download model files
!wget https://raw.githubusercontent.com/richzhang/colorization/master/models/colorization_deploy_v2.prototxt
!wget http://eecs.berkeley.edu/~rich.zhang/projects/2016_colorization/files/demo_v2/colorization_release_v2.caffemodel
!wget https://raw.githubusercontent.com/richzhang/colorization/master/resources/pts_in_hull.npy

!wget -O pts_in_hull.npy https://huggingface.co/spaces/akhaliq/DeOldify/resolve/main/pts_in_hull.npy

import cv2
import numpy as np
from google.colab.patches import cv2_imshow

# Load model
net = cv2.dnn.readNetFromCaffe('colorization_deploy_v2.prototxt',
                               'colorization_release_v2.caffemodel')
pts = np.load('pts_in_hull.npy')

# Inject cluster centers
class8 = net.getLayerId('class8_ab')
conv8 = net.getLayerId('conv8_313_rh')
net.getLayer(class8).blobs = [pts.transpose().reshape(2, 313, 1, 1)]
net.getLayer(conv8).blobs = [np.full([1, 313], 2.606, dtype="float32")]

# Upload and read your image
from google.colab import files
uploaded = files.upload()

import os
filename = next(iter(uploaded))
bw = cv2.imread(filename)
bw = cv2.cvtColor(bw, cv2.COLOR_BGR2GRAY)
bw = cv2.cvtColor(bw, cv2.COLOR_GRAY2BGR)

# Convert to Lab and extract L channel
bw = bw.astype("float32") / 255.0
lab = cv2.cvtColor(bw, cv2.COLOR_BGR2Lab)
L = lab[:, :, 0]

# Resize and prepare input
resized_L = cv2.resize(L, (224, 224))
net_input = cv2.dnn.blobFromImage(resized_L)
net.setInput(net_input)
ab = net.forward()[0, :, :, :].transpose((1, 2, 0))

# Resize ab and merge with original L
ab = cv2.resize(ab, (bw.shape[1], bw.shape[0]))
colorized_lab = np.concatenate((L[:, :, np.newaxis], ab), axis=2)
colorized = cv2.cvtColor(colorized_lab, cv2.COLOR_Lab2BGR)
colorized = (colorized * 255).astype("uint8")

# Show and save result
cv2_imshow(colorized)
cv2.imwrite('/content/colorized_output.jpg', colorized)

import cv2
import numpy as np
from google.colab.patches import cv2_imshow

# Load model
net = cv2.dnn.readNetFromCaffe('colorization_deploy_v2.prototxt',
                               'colorization_release_v2.caffemodel')
pts = np.load('pts_in_hull.npy')

# Inject cluster centers
class8 = net.getLayerId('class8_ab')
conv8 = net.getLayerId('conv8_313_rh')
net.getLayer(class8).blobs = [pts.transpose().reshape(2, 313, 1, 1)]
net.getLayer(conv8).blobs = [np.full([1, 313], 2.606, dtype="float32")]

# Load grayscale image
bw = cv2.imread('/content/unnamed.jpg')
bw = cv2.cvtColor(bw, cv2.COLOR_BGR2GRAY)
bw = cv2.cvtColor(bw, cv2.COLOR_GRAY2BGR)

# Convert to Lab and extract L channel
bw = bw.astype("float32") / 255.0
lab = cv2.cvtColor(bw, cv2.COLOR_BGR2Lab)
L = lab[:, :, 0]

# Resize and prepare input
resized_L = cv2.resize(L, (224, 224))
net_input = cv2.dnn.blobFromImage(resized_L)
net.setInput(net_input)
ab = net.forward()[0, :, :, :].transpose((1, 2, 0))

# Resize ab and merge with original L
ab = cv2.resize(ab, (bw.shape[1], bw.shape[0]))
colorized_lab = np.concatenate((L[:, :, np.newaxis], ab), axis=2)
colorized = cv2.cvtColor(colorized_lab, cv2.COLOR_Lab2BGR)
colorized = (colorized * 255).astype("uint8")

# Show result
cv2_imshow(colorized)
cv2.imwrite('/content/colorized_output.jpg', colorized)

import cv2
import numpy as np

# Load the model
net = cv2.dnn.readNetFromCaffe('colorization_deploy_v2.prototxt',
                               'colorization_release_v2.caffemodel')
pts = np.load('pts_in_hull.npy')

# Inject cluster centers
class8 = net.getLayerId('class8_ab')
conv8 = net.getLayerId('conv8_313_rh')
net.getLayer(class8).blobs = [pts.transpose().reshape(2, 313, 1, 1)]
net.getLayer(conv8).blobs = [np.full([1, 313], 2.606, dtype="float32")]

# Load grayscale image
bw = cv2.imread('unnamed.jpg')
bw = cv2.cvtColor(bw, cv2.COLOR_BGR2GRAY)
bw = cv2.cvtColor(bw, cv2.COLOR_GRAY2BGR)

# Convert to Lab and extract L channel
bw = bw.astype("float32") / 255.0
lab = cv2.cvtColor(bw, cv2.COLOR_BGR2Lab)
L = lab[:, :, 0]

# Resize and prepare input
resized_L = cv2.resize(L, (224, 224))
net_input = cv2.dnn.blobFromImage(resized_L)
net.setInput(net_input)
ab = net.forward()[0, :, :, :].transpose((1, 2, 0))

# Resize ab and merge with original L
ab = cv2.resize(ab, (bw.shape[1], bw.shape[0]))
colorized_lab = np.concatenate((L[:, :, np.newaxis], ab), axis=2)
colorized = cv2.cvtColor(colorized_lab, cv2.COLOR_Lab2BGR)
colorized = (colorized * 255).astype("uint8")

# Save result
cv2.imwrite('colorized_output.jpg', colorized)

import cv2
import numpy as np

# Load the pre-trained model
net = cv2.dnn.readNetFromCaffe('colorization_deploy_v2.prototxt',
                               'colorization_release_v2.caffemodel')
pts = np.load('pts_in_hull.npy')

# Add cluster centers to the model
class8 = net.getLayerId('class8_ab')
conv8 = net.getLayerId('conv8_313_rh')
net.getLayer(class8).blobs = [pts.transpose().reshape(2, 313, 1, 1)]
net.getLayer(conv8).blobs = [np.full([1, 313], 2.606, dtype="float32")]

# Load your black & white image
bw_image = cv2.imread('/content/unnamed.jpg')
bw_image = cv2.cvtColor(bw_image, cv2.COLOR_BGR2GRAY)
bw_image = cv2.cvtColor(bw_image, cv2.COLOR_GRAY2BGR)

# Convert to Lab color space
normalized = bw_image.astype("float32") / 255.0
lab = cv2.cvtColor(normalized, cv2.COLOR_BGR2Lab)
L = lab[:, :, 0]

# Resize and prepare input
resized_L = cv2.resize(L, (224, 224))
net_input = cv2.dnn.blobFromImage(resized_L)
net.setInput(net_input)
ab_output = net.forward()[0, :, :, :].transpose((1, 2, 0))

# Resize ab channels and merge with original L
ab_output_us = cv2.resize(ab_output, (bw_image.shape[1], bw_image.shape[0]))
colorized_lab = np.concatenate((L[:, :, np.newaxis], ab_output_us), axis=2)
colorized_bgr = cv2.cvtColor(colorized_lab, cv2.COLOR_Lab2BGR)
colorized_bgr = (colorized_bgr * 255).astype("uint8")

# Save the result
cv2.imwrite('colorized_output.jpg', colorized_bgr)
print("Colorized image saved as 'colorized_output.jpg'")

#Grey Scale Universal
a=Image.open("/content/unnamed.jpg")
raw=a.load()
width=a.width
height=a.height
for w in range (width):
  for h in range (height):
    r,g,b=raw[w,h]
    r=r*.299
    g=g*.587
    b=b*.114
    s=round(r+g+b)
    raw[w,h]=(s,s,s)



import cv2
import numpy as np

# Load pre-trained model files
prototxt = "colorization_deploy_v2.prototxt"
model = "colorization_release_v2.caffemodel"
points = "pts_in_hull.npy"

net = cv2.dnn.readNetFromCaffe(prototxt, model)
pts = np.load(points).transpose().reshape(2, 313, 1, 1)
net.getLayer(net.getLayerId("class8_ab")).blobs = [pts.astype("float32")]
net.getLayer(net.getLayerId("conv8_313_rh")).blobs = [np.full([1, 313], 2.606, dtype="float32")]

# Load grayscale image
img = cv2.imread("/content/Grandfather & Mother.jpeg")
img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
img_gray = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2BGR)

# Convert to LAB
lab = cv2.cvtColor(img_gray, cv2.COLOR_BGR2LAB)
L = lab[:, :, 0]

# Prepare input
L = cv2.resize(L, (224, 224))
L -= 50
net.setInput(cv2.dnn.blobFromImage(L))
ab = net.forward()[0, :, :, :].transpose((1, 2, 0))

# Resize ab channels
ab = cv2.resize(ab, (img.shape[1], img.shape[0]))

# Combine L + ab
L = lab[:, :, 0]
color_lab = np.concatenate((L[:, :, np.newaxis], ab), axis=2)

# Convert LAB ‚Üí BGR
color_bgr = cv2.cvtColor(color_lab.astype("uint8"), cv2.COLOR_LAB2BGR)

# Save output
cv2.imwrite("colored.jpg", color_bgr)

print("Colorization completed!")

!pip install deoldify
!pip install ffmpeg-python

from deoldify import device
from deoldify.device import *
from deoldify.visualize import *

colorizer = get_image_colorizer(artistic=True)
colorizer.plot_transformed_image(source='/content/Grandfather & Mother.jpeg', render_factor=35, display_render_factor=True)

!pip install deoldify
!pip install ffmpeg-python
!pip install wandb

# Disable wandb login
import os
os.environ["WANDB_MODE"] = "dryrun"

from deoldify import device
from deoldify.device import *
from deoldify.visualize import *

# Use GPU
device.set(device_type='cuda')
torch.backends.cudnn.benchmark = True

# Load Colorizer
colorizer = get_image_colorizer(artistic=True)

"""# **11.05.2025**"""

data = input()
data_new = int(data, type(data))
print(data_new)

data=456
print(data, type(data))

data = input()
print(data, type(data))
data1 = int(data)
print(data1,type(data))

message="Hi Sethu, you withdrawn {} from {}, your available balance is {}"
amount=input()
bank=input("enter the bank")
amount1=input()
print(message.format(amount,bank,amount1))

name=("sethu","rajan","bhujam", "kujali", "shyam", "sankaran", "veerakutty","hari","ramanakan")
name
print(name,type(name))
name.append(c\)

name.clear()

name=("a","b","c")
name.append("h")
name

name=["a","b","c","d"]
name.append("e")
name

name=["a","a","b","","c","d","e"]
name.insert(3, "j")
name
name.append("z")
name
name.count("a"

message="hello sethu, you withdrawn {}, from {},and your available balance is {}"
amount=input()
bank=input()
amount1=input()
print(message.format(amount,bank,amount1))

name = "sethuraman"
name.upper()
a="sethuraman"
a.upper()
data="ramasethu"
data.upper()
len(data)
dir(str)
dir(int)

message="hello sethu,you withdrawan {}, from {},at ATM,"

now = datetime.now()
print("Current date & time", now)

import datetime
now = datetime.now
print("current date & time :", now)

current_time = now.strftime("%Y-%m-%d %H:%M:%S")
print(current_time)

datetime.now()

import datetime
x = datetime.datetime.now()
print(x)

import datetime
x = datetime.datetime.now()
print(x)

import datetime
x = datetime.datetime.now()
print(x)

message="hello sethu,you have withdrawan {},from {}, x and your available balance is {}"
amount=input()
name=input()
import datetime
x=datetime.datetime.now()
amount1=input()
print(message.format(amount,name,x,amount1))

message="hello sethu,you have withdrawan {},from {}, and your available balance is {}"
amount=input()
name=input()
amount1=input()
print(message.format(amount,name,amount1))

message="Hi Sethu, you withdrawn {} from {}, your available balance is {}"
amount=input()
bank=input("enter the bank")
amount1=input()
print(message.format(amount,bank,amount1))

message="hello I am {}, I'm from {}, my salary is {}, my age is {}"
name = input("enter your name")
place = input("enter your place")
salary = int(input("enter your salary"))
age = input()
print(message.format(name,place,salary,age))

message = "hello I am {}, I'm from {}, my salary is {}, my age is {}"
name = input("enter your name")
place = input("enter your place")
salary = int(input("enter your salary"))
age = input()

print(message.format(place,name,salary,age))

message = "hello, I'm {}, my age is {}"
name = input("enter your name :")
age = input("enter your age :")
print(message.format(age,name))

name = "sethu raman"
name.split("s m")
name.upper()

id(name)

name = input("enter your name :")
dob = input("enter your date of birth :")
amount = input("enter your valid credit card number :")
password = name[:4]+dob[-4:]+amount[-4:]
print(password)

"""**Vibeathon 14.11.2025**

"""

import os
import torch
import numpy as np
from moviepy.editor import VideoFileClip
from transformers import pipeline
from sentence_transformers import SentenceTransformer, util

# 1. Extract audio from video
def extract_audio(video_path, audio_path="/content/HBYW2907.mp3"):
    clip = VideoFileClip(video_path)
    # Explicitly set codec for audio extraction, and change output extension to .mp3
    clip.audio.write_audiofile(audio_path, codec='mp3', verbose=False, logger=None)
    return audio_path

# 2. Transcribe audio using Whisper (via HuggingFace pipeline)
def transcribe_audio(audio_path):
    print("üß† Transcribing audio with Whisper...")
    whisper = pipeline("automatic-speech-recognition", model="openai/whisper-base")
    result = whisper(audio_path)
    return result['text']

# 3. Chunk transcript into sentence groups
def chunk_text(text, chunk_size=2):
    sentences = [s.strip() for s in text.split('.') if s.strip()]
    return ['. '.join(sentences[i:i+chunk_size]) for i in range(0, len(sentences), chunk_size)]

# 4. Embed and compare
def compute_similarity(title, transcript_chunks):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    title_embedding = model.encode(title, convert_to_tensor=True)
    chunk_embeddings = model.encode(transcript_chunks, convert_to_tensor=True)
    similarities = [util.cos_sim(title_embedding, chunk).item() for chunk in chunk_embeddings]
    return similarities

# 5. Score and verdict
def score_similarity(similarities):
    avg_score = np.mean(similarities)
    verdict = (
        "‚úÖ Strong Match" if avg_score > 0.6 else
        "‚ö†Ô∏è Partial Match" if avg_score > 0.3 else
        "‚ùå Mismatch or Clickbait"
    )
    return avg_score, verdict

# 6. Full pipeline
def analyze_video(video_path, title):
    print("üîÑ Extracting audio from video...")
    audio_path = extract_audio(video_path)

    transcript = transcribe_audio(audio_path)
    print("‚úÇÔ∏è Chunking transcript...")
    chunks = chunk_text(transcript)

    print("üîç Computing semantic similarity...")
    similarities = compute_similarity(title, chunks)

    print("üìä Scoring...")
    avg_score, verdict = score_similarity(similarities)

    print("\nüé¨ Video Title:", title)
    print("üìù Transcript Preview:", transcript[:300], "...")
    print(f"üìà Average Similarity Score: {avg_score:.2f}")
    print(f"üßæ Verdict: {verdict}")

    print("\nüîé Chunk-wise Similarities:")
    for i, (chunk, score) in enumerate(zip(chunks, similarities)):
        print(f"Chunk {i+1}: {score:.2f} ‚Üí {chunk}")

    os.remove(audio_path)  # Clean up

# 7. Run the script
if __name__ == "__main__":
    # Replace 'your_video.mp4' with the actual path to your video file.
    # For example, if you uploaded a video named 'my_awesome_video.mp4' to your Colab environment,
    # you would use: video_file = "/content/my_awesome_video.mp4"
    video_file = "/content/HBYW2907.MOV"  # Replace with your video file path
    video_title = "Bomma Kolu"  # Replace with your video title
    analyze_video(video_file, video_title)

"""# Insurance Premium Calculation 15.11.2025"""

import pandas as pd
import numpy as np
import missingno as msno

train_data = pd.read_excel("/content/final_train_data for ML.xlsx")

test_data = pd.read_excel("/content/test_final for ML.xlsx")

train_data.info()

msno.bar(train_data)

train_data['Policy Start Date'] = pd.to_datetime(train_data['Policy Start Date'])

train_data['Policy Year'] = train_data['Policy Start Date'].dt.year
train_data['Policy Month'] = train_data['Policy Start Date'].dt.month
train_data['Policy Date'] = train_data['Policy Start Date'].dt.day

train_data.head()

train_data = train_data.drop(columns = ["id"], errors='ignore')
train_data = train_data.drop(columns = ["Policy Start Date"], errors='ignore')

train_data.head()

test_data.info()

test_data = test_data.drop(columns = ["id"], errors='ignore')
test_data = test_data.drop(columns = ["Policy Start Date"], errors='ignore')
test_data = test_data.drop(columns = ["Policy_hour"],errors = 'ignore')
test_data = test_data.drop(columns = ["Policy_minute"], errors = 'ignore')
test_data = test_data.drop(columns= ["Policy_second"], errors = 'ignore')

test_data.head()

corr = train_data.corr()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10,10))
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.show()

plt.figure(figsize=(20,20))
sns.heatmap(corr, annot=True, cmap='magma')
plt.show()

plt.figure(figsize=(20,20))
sns.heatmap(corr, annot=True, cmap='inferno')
plt.show()

from sklearn.preprocessing import StandardScaler

scalar = StandardScaler()

X = train_data.drop(columns = ["Premium Amount"], axis = 1)
y = train_data["Premium Amount"]

X.shape,y.shape

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

scaled_train_data = scalar.fit_transform(X_train)
scaled_test_data = scalar.transform(X_test)

from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
model = RandomForestRegressor(n_estimators=100, max_depth = 10, random_state=42)
model.fit(scaled_train, y_train)
pred = model.predict(scaled_test)
mae = mean_absolute_error(y_test, pred)
mse = mean_squared_error(y_test, pred)
r2 = r2_score(y_test, pred)

# Print results
print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("R¬≤ Score:", r2)

from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(scaled_train_data, y_train)
pred = model.predict(scaled_test_data)
mae = mean_absolute_error(y_test, pred)
mse = mean_squared_error(y_test, pred)
r2 = r2_score(y_test, pred)

# Print results
print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("R¬≤ Score:", r2)

from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
model = RandomForestRegressor(n_estimators=300, max_depth = 10, random_state=42)
model.fit(scaled_train_data, y_train)
pred = model.predict(scaled_test_data)
mae = mean_absolute_error(y_test, pred)
mse = mean_squared_error(y_test, pred)
r2 = r2_score(y_test, pred)

# Print results
print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("R¬≤ Score:", r2)

from sklearn.tree import DecisionTreeRegressor
model = DecisionTreeRegressor(max_depth = 10, random_state=42)
model.fit(scaled_train_data, y_train)
pred = model.predict(scaled_test_data)
mae = mean_absolute_error(y_test, pred)
mse = mean_squared_error(y_test, pred)
r2 = r2_score(y_test, pred)

# Print results
print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("R¬≤ Score:", r2)

from sklearn.svm import SVR
model = SVR()
model.fit(scaled_train_data, y_train)
pred = model.predict(scaled_test_data)
mae = mean_absolute_error(y_test, pred)
mse = mean_squared_error(y_test, pred)
r2 = r2_score(y_test, pred)
# Print results
print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("R¬≤ Score:", r2)

from sklearn.ensemble import GradientBoostingRegressor
model = GradientBoostingRegressor()
model.fit(scaled_train_data, y_train)
pred = model.predict(scaled_test_data)
mae = mean_absolute_error(y_test, pred)
mse = mean_squared_error(y_test, pred)
r2 = r2_score(y_test, pred)
# Print results
print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("R¬≤ Score:", r2)

"""# Swiggy Recommendation 15.11.2025"""

import pandas as pd
import numpy as np

swiggy = pd.read_csv("/content/swiggy_cleaned.csv")

swiggy.info()

import missingno as msno

msno.bar(swiggy)

import missingno as msno

# Bar chart
msno.bar(swiggy)

# Print missing values
print("\nMissing values per column:")
print(swiggy.isnull().sum())

swiggy = swiggy.drop(columns = ["id"])

swiggy.info()

swiggy = swiggy.drop(columns = ["lic_no", "link", "menu"])

swiggy.head()

swiggy['name'].nunique()

swiggy['city'].nunique()

swiggy['cuisine'].unique()

swiggy['city'].unique()

print(swiggy['city'].unique())

for i in swiggy.columns:
    if swiggy[i].dtype == object:
        print(i)

for i in swiggy.columns:
     if swiggy[i].isnull().sum():
         print(i)

swiggy["name"].isna().sum()

swiggy[['address','rating','rating_count','cost','cuisine']].isna().sum()

swiggy[['address','rating','rating_count','cost','cuisine']].fillna(0, inplace=True)

columns_to_fill = ['name','address','rating','rating_count','cost','cuisine']
swiggy.loc[:, columns_to_fill] = swiggy[columns_to_fill].ffill()

swiggy.info()

swiggy[['name','address','rating','rating_count','cost','cuisine']].isna().sum()

import pandas as pd


# One-hot encode
swiggy = pd.get_dummies(swiggy, columns=['city'])

print(swiggy)

# One-hot encode
swiggy = pd.get_dummies(swiggy, columns=['cuisine'])

print(swiggy)

import pickle

# Save to pickle file
with open("city.pkl", "wb") as f:   # 'wb' = write binary
    pickle.dump(city_df_encoded, f)

print("Pickle file -City created successfully!")

import pickle

# Save to pickle file
with open("cuisine.pkl", "wb") as f:   # 'wb' = write binary
    pickle.dump(cuisine_df_encoded, f)

print("Pickle file -Cuisine created successfully!")

swiggy.info()

swiggy.head()

result = pd.concat([city_df_encoded, cuisine_df_encoded], axis=1)
print(result)

# One-hot encode 'city' column
city_df_encoded = pd.get_dummies(swiggy, columns=['city'])

print(df_encoded)

for i in swiggy.columns:
    if swiggy[i].dtype == object:
        print(i)

swiggy.info()

swiggy.head()

from sklearn.preprocessing import LabelEncoder
LE = LabelEncoder()
swiggy['name'] = LE.fit_transform(swiggy['name'])
swiggy['address'] = LE.fit_transform(swiggy['address'])
swiggy['rating'] = LE.fit_transform(swiggy['rating'])
swiggy['rating_count'] = LE.fit_transform(swiggy['rating_count'])

swiggy.head()

# Remove 'Rs.' and convert to numeric
swiggy['cost'] = swiggy['cost'].str.replace('‚Çπ', '', regex=False).astype(int)

swiggy.tail()

swiggy.info()

for i in swiggy.columns:
    if swiggy[i].dtype == object:
        print(i)

swiggy.to_csv("swiggy_encoded.csv", index=False)

"""# Smart Premium Workings 18.11.2025"""

import pandas as pd
import numpy as np
import missingno as msno

train = pd.read_excel("/content/final_train_data for ML.xlsx")

test = pd.read_excel("/content/test_final for ML.xlsx")

train.info()

train['Policy Start Date'] = pd.to_datetime(train['Policy Start Date'])

train['Policy Year'] = train['Policy Start Date'].dt.year
train['Policy Month'] = train['Policy Start Date'].dt.month
train['Policy Date'] = train['Policy Start Date'].dt.day

train.info()

train['id'].drop(columns = ["id"], axis = 1)
train['Policy Start Date'].drop(columns = ["Policy Start Date"], axis = 1)

train["id"].drop(columns = ["id"], axis = 1)

train.info()

train = train.drop(columns = ["id"], axis=1)
train = train.drop(columns = ["Policy Start Date"], axis=1)

train.info()

test.info()

test = test.drop(columns = ["id"], axis=1)
test = test.drop(columns = ["Policy_hour"], axis=1)
test = test.drop(columns = ["Policy_minute"], axis=1)
test = test.drop(columns = ["Policy_second"], axis=1)

test.info()

train.skew()

train.kurtosis()

train.corr()

train.corr()["Premium Amount"].sort_values(ascending=False)

train['Annual Income'].corr(train['Premium Amount'])

train.corr()["Previous Claims"].sort_values(ascending=False)

import seaborn as sns
import matplotlib.pyplot as plt

sns.regplot(x=train['Age'], y=train['Premium Amount'])
plt.xlabel('Age')
plt.ylabel('Premium Amount')
plt.title('Correlation: Age vs Premium')
plt.show()

sns.regplot(x=train['Age'], y=train['Premium Amount'])

import seaborn as sns

corr_value = train[['Age', 'Premium Amount']].corr()

sns.heatmap(corr_value, annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

from sklearn.preprocessing import StandardScaler

scalar = StandardScaler()

X = train.drop(columns = ["Premium Amount"], axis = 1)
y = train["Premium Amount"]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

scaled_train = scalar.fit_transform(X_train)
scaled_test = scalar.transform(X_test)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
model = RandomForestRegressor(n_estimators=100, max_depth = 10, random_state=42)
model.fit(scaled_train, y_train)
pred = model.predict(scaled_test)
mae = mean_absolute_error(y_test, pred)
mse = mean_squared_error(y_test, pred)
r2 = r2_score(y_test, pred)

# Print results
print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("R¬≤ Score:", r2)

from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(scaled_train, y_train)
pred = model.predict(scaled_test)
mae = mean_absolute_error(y_test, pred)
mse = mean_squared_error(y_test, pred)
r2 = r2_score(y_test, pred)

# Print results
print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("R¬≤ Score:", r2)

from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
model = RandomForestRegressor(n_estimators=300, max_depth = 10, random_state=42)
model.fit(scaled_train, y_train)
pred = model.predict(scaled_test)
mae = mean_absolute_error(y_test, pred)
mse = mean_squared_error(y_test, pred)
r2 = r2_score(y_test, pred)

# Print results
print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("R¬≤ Score:", r2)

from sklearn.tree import DecisionTreeRegressor
model = DecisionTreeRegressor(max_depth = 10, random_state=42)
model.fit(scaled_train, y_train)
pred = model.predict(scaled_test)
mae = mean_absolute_error(y_test, pred)
mse = mean_squared_error(y_test, pred)
r2 = r2_score(y_test, pred)

# Print results
print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("R¬≤ Score:", r2)

from sklearn.svm import SVR
model = SVR()
model.fit(scaled_train, y_train)
pred = model.predict(scaled_test)
mae = mean_absolute_error(y_test, pred)
mse = mean_squared_error(y_test, pred)
r2 = r2_score(y_test, pred)
# Print results
print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("R¬≤ Score:", r2)

from sklearn.ensemble import GradientBoostingRegressor
model = GradientBoostingRegressor()
model.fit(scaled_train, y_train)
pred = model.predict(scaled_test)
mae = mean_absolute_error(y_test, pred)
mse = mean_squared_error(y_test, pred)
r2 = r2_score(y_test, pred)
# Print results
print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("R¬≤ Score:", r2)

train.to_csv("train_ml", index=False)
test.to_csv("test_ml", index=False)

"""# Swiggy Recommendation Workings 18.11.2025"""

import pandas as pd
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder

swiggy = pd.read_csv("/content/swiggy.csv")

swiggy.info()

for i in swiggy.columns:
     if swiggy[i].isnull().sum():
         print(i)

swiggy = swiggy.drop(columns = ["id"])
swiggy = swiggy.drop(columns = ["lic_no"])
swiggy = swiggy.drop(columns = ["link"])
swiggy = swiggy.drop(columns = ["menu"])

ohe_cols = ["city", "cuisine"]                   # One-Hot Encode
all_cat = swiggy.select_dtypes(include=["object"]).columns.tolist()
other_cat = [c for c in all_cat if c not in ohe_cols]   # Other categorical columns

ohe = OneHotEncoder(sparse_output=False, drop="first")  # drop first to avoid dummy trap
ohe_matrix = ohe.fit_transform(swiggy[ohe_cols])
ohe_df = pd.DataFrame(ohe_matrix, columns=ohe.get_feature_names_out(ohe_cols))

ord_enc = OrdinalEncoder()
ordinal_matrix = ord_enc.fit_transform(swiggy[other_cat])
ordinal_df = pd.DataFrame(ordinal_matrix, columns=other_cat)

num_cols = swiggy.select_dtypes(include=["int64", "float64"]).columns.tolist()
num_df = swiggy[num_cols].reset_index(drop=True)

final_df = pd.concat([num_df.reset_index(drop=True),
                      ordinal_df.reset_index(drop=True),
                      ohe_df.reset_index(drop=True)], axis=1)

print("\nFinal processed data shape:", final_df.shape)
print(final_df.head())

final_df.info()

final_df.head()

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Assume final_df is your fully encoded dataframe from previous step

# ---------------------------------------------
# 1. Scaling numeric features
# ---------------------------------------------
scaler = StandardScaler()
scaled_data = scaler.fit_transform(final_df)

# ---------------------------------------------
# 2. Applying K-Means clustering
# ---------------------------------------------
kmeans = KMeans(n_clusters=4, random_state=42)     # You can change cluster count
cluster_labels = kmeans.fit_predict(scaled_data)

# ---------------------------------------------
# 3. Add cluster labels back to dataframe
# ---------------------------------------------
final_df["cluster"] = cluster_labels

# ---------------------------------------------
# 4. Show final result
# ---------------------------------------------
print("Final DataFrame shape:", final_df.shape)
print(final_df.head())

# ---------------------------------------------
# 5. Save processed file (optional)
# ---------------------------------------------
final_df.to_csv("processed_final_data.csv", index=False)
print("Saved as processed_final_data.csv")

"""# Second approach 18.11.2025 Pickle separately"""

import pandas as pd

df = pd.read_csv("/content/swiggy_cleaned_data.csv")

df.info()

df.head()

df["name"].unique()

# columns you want to One-Hot Encode
ohe_cols = ["city", "cuisine"]

# find other categorical columns
other_cat_cols = df.select_dtypes(include=["object"]).columns.tolist()

# remove the OHE columns
other_cat_cols = [col for col in other_cat_cols if col not in ohe_cols]

# find numeric columns
numeric_cols = df.select_dtypes(include=["int64", "float64"]).columns.tolist()

from sklearn.preprocessing import OneHotEncoder
import pickle

ohe = OneHotEncoder(sparse_output=True, handle_unknown="ignore")

city_cuisine_encoded = ohe.fit_transform(df[ohe_cols])

# Save encoded sparse matrix
pickle.dump(city_cuisine_encoded, open("city_cuisine_encoded.pkl", "wb"))

# Save the encoder
pickle.dump(ohe, open("ohe_city_cuisine.pkl", "wb"))

from sklearn.preprocessing import OrdinalEncoder

ord_enc = OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)

other_cat_encoded = ord_enc.fit_transform(df[other_cat_cols])

# Save encoded matrix
pickle.dump(other_cat_encoded, open("other_cat_encoded.pkl", "wb"))

# Save encoder
pickle.dump(ord_enc, open("ordinal_encoder.pkl", "wb"))

numeric_data = df[numeric_cols].values

# Save numeric values
pickle.dump(numeric_data, open("numeric_data.pkl", "wb"))

from scipy.sparse import hstack
import pickle

city_cuisine = pickle.load(open("city_cuisine_encoded.pkl", "rb"))
other_cat = pickle.load(open("other_cat_encoded.pkl", "rb"))
numeric = pickle.load(open("numeric_data.pkl", "rb"))

# convert numeric to sparse before hstack
from scipy.sparse import csr_matrix
numeric_sparse = csr_matrix(numeric)

X_final = hstack([city_cuisine, other_cat, numeric_sparse])

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from scipy.sparse import csr_matrix, hstack
import pickle

# Load df from the original csv, as implied by the preceding cells in this section
df = pd.read_csv("/content/swiggy.csv")

# Clean and define the target variable (assuming 'cost' is the target)
# The 'cost' column was previously cleaned in cell ujPO0idfcz7Z by removing '‚Çπ' and converting to int.
# This assumes the 'df' here refers to the initial loaded dataframe before any drops.
df['cost'] = df['cost'].astype(str).str.replace('‚Çπ', '', regex=False)
# Handle potential non-numeric values after stripping currency symbol, e.g., 'nan' or empty strings
df['cost'] = pd.to_numeric(df['cost'], errors='coerce').fillna(df['cost'].median()).astype(int)

y = df["cost"]

# Load the pre-encoded feature matrices from pickle files
# Note: These pickled feature matrices were likely created BEFORE the 'cost' column was removed from features.
# If 'cost' is the intended target, it should have been excluded from the feature encoding process
# (i.e., from `other_cat_cols`) when `other_cat_encoded.pkl` was generated to avoid data leakage.
# For now, we are addressing the immediate KeyError and will point out the potential data leakage.

city_cuisine = pickle.load(open("city_cuisine_encoded.pkl", "rb"))
other_cat = pickle.load(open("other_cat_encoded.pkl", "rb"))
numeric = pickle.load(open("numeric_data.pkl", "rb"))

# convert numeric to sparse before hstack
numeric_sparse = csr_matrix(numeric)

X_final = hstack([city_cuisine, other_cat, numeric_sparse])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42)

# Initialize and train the model
model = RandomForestRegressor()
model.fit(X_train, y_train)

# save model
pickle.dump(model, open("premium_model.pkl","wb"))

import pickle
from scipy.sparse import hstack, csr_matrix
import pandas as pd

# Load original dataset (cleaned)
clean_df = pd.read_csv("/content/swiggy_cleaned.csv")

# Load encoded features
city_cuisine = pickle.load(open("city_cuisine_encoded.pkl","rb"))
other_cat = pickle.load(open("other_cat_encoded.pkl","rb"))
numeric = pickle.load(open("numeric_data.pkl","rb"))

# Convert numeric to sparse
numeric_sparse = csr_matrix(numeric)

# Final encoded matrix
X_final = hstack([city_cuisine, other_cat, numeric_sparse])

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=12, random_state=42)
clusters = kmeans.fit_predict(X_final)

# Save cluster labels
clean_df["cluster"] = clusters

import pandas as pd

train = pd.read_excel("/content/final_train_data for ML.xlsx")

train.info()

train["Policy Start Date"] = pd.to_datetime(train["Policy Start Date"])

train["Policy Start Date"].dt.year

train["Policy date"] = train["Policy Start Date"].dt.date
train["Policy month"] = train["Policy Start Date"].dt.month
train["Policy year"] = train["Policy Start Date"].dt.year

train = train.drop(columns = ["Policy Start Date"], axis=1)

train.info()

train = train.drop(columns = ["id"], axis = 1 )

train.head()

train.to_csv("train_ml.csv",index = False)

train["Policy day"] = train["Policy date"].apply(lambda x: x.day)

train.head()

train = train.drop(columns = ["Policy date"], axis = 1)

train.head()

train.to_excel("train_ml.xlsx", index = False)

train.to_csv("train_ml.csv", index = False)

import pandas as pd

train = pd.read_excel("/content/final_train_data for ML.xlsx")

train.head()

train['Policy day'] = train['Policy Start Date'].dt.day
train['Policy month'] = train['Policy Start Date'].dt.month
train['Policy year'] = train["Policy Start Date"].dt.year

train = train.drop(columns = ['id'])
train = train.drop(columns = ['Policy Start Date'])

train.info()

train.to_csv("train_ml.csv", index = False)

"""# Swiggy Recommendation workings 22.11.2025"""

import pandas as pd
import numpy as np

swiggy_df = pd.read_csv("/content/swiggy_sample_cleaned.csv")

swiggy_df.info()

swiggy_df = swiggy_df.drop(columns = ["id"])
swiggy_df = swiggy_df.drop(columns = ["lic_no"])
swiggy_df = swiggy_df.drop(columns = ["link"])
swiggy_df = swiggy_df.drop(columns = ["menu"])

swiggy_df.info()

from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder

ohe_cols = ["city", "cuisine"]                   # One-Hot Encode
all_cat = swiggy_df.select_dtypes(include=["object"]).columns.tolist()
other_cat = [c for c in all_cat if c not in ohe_cols]   # Other categorical columns

ohe = OneHotEncoder(sparse_output=False, drop="first")  # drop first to avoid dummy trap
ohe_matrix = ohe.fit_transform(swiggy_df[ohe_cols])
ohe_df = pd.DataFrame(ohe_matrix, columns=ohe.get_feature_names_out(ohe_cols))

ord_enc = OrdinalEncoder()
ordinal_matrix = ord_enc.fit_transform(swiggy_df[other_cat])
ordinal_df = pd.DataFrame(ordinal_matrix, columns=other_cat)

num_cols = swiggy_df.select_dtypes(include=["int64", "float64"]).columns.tolist()
num_df = swiggy_df[num_cols].reset_index(drop=True)

final_df = pd.concat([num_df.reset_index(drop=True),
                      ordinal_df.reset_index(drop=True),
                      ohe_df.reset_index(drop=True)], axis=1)

print("\nFinal processed data shape:", final_df.shape)
print(final_df.head())

final_df.info()

final_df.head()

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Assume final_df is your fully encoded dataframe from previous step

# ---------------------------------------------
# 1. Scaling numeric features
# ---------------------------------------------
scaler = StandardScaler()
scaled_data = scaler.fit_transform(final_df)

# ---------------------------------------------
# 2. Applying K-Means clustering
# ---------------------------------------------
kmeans = KMeans(n_clusters=4, random_state=42)     # You can change cluster count
cluster_labels = kmeans.fit_predict(scaled_data)

# ---------------------------------------------
# 3. Add cluster labels back to dataframe
# ---------------------------------------------
final_df["cluster"] = cluster_labels

# ---------------------------------------------
# 4. Show final result
# ---------------------------------------------
print("Final DataFrame shape:", final_df.shape)
print(final_df.head())

# ---------------------------------------------
# 5. Save processed file (optional)
# ---------------------------------------------
final_df.to_csv("processed_final_data.csv", index=False)
print("Saved as processed_final_data.csv")

df = pd.read_csv("/content/processed_final_data.csv")

df.info()

df.head()

from sklearn.metrics.pairwise import cosine_similarity

num_df = df.select_dtypes(include=['int64', 'float64'])
cos_sim = cosine_similarity(num_df)

def recommend_restaurants(name, top_n=5):
    if name not in df["address"].values:
        return f"{name} not found!"

    idx = df.index[df["address"] == name][0]

    scores = list(enumerate(cos_sim[idx]))
    scores = sorted(scores, key=lambda x: x[1], reverse=True)

    top_matches = scores[1:top_n+1]
    return df.iloc[[i[0] for i in top_matches]]["address"].tolist()

def recommend_by_cosine(restaurant_name, top_n=5):

    if restaurant_name not in df['name'].values:
        return f"{restaurant_name} not found in dataset!"

    # Get index of selected restaurant
    idx = df.index[df['name'] == restaurant_name][0]

    # Get similarity scores
    scores = list(enumerate(cos_sim[idx]))

    # Sort by highest similarity
    scores = sorted(scores, key=lambda x: x[1], reverse=True)

    # Get top N similar restaurants (skip itself)
    top_results = scores[1:top_n+1]

    return df.iloc[[i[0] for i in top_results]]['name'].tolist()

recommend_by_cosine("Dominos Pizza")

recommend_by_cosine("LUSSI PLUS")

def recommend_by_cluster(restaurant_name, top_n=5):

    if restaurant_name not in df['name'].values:
        return f"{restaurant_name} not found in dataset!"

    # Find cluster of the restaurant
    cluster_id = df.loc[df['name'] == restaurant_name, 'cluster'].values[0]

    # Get all restaurants from same cluster
    cluster_members = df[df['cluster'] == cluster_id]

    # Return first N
    return cluster_members['name'].head(top_n).tolist()

recommend_by_cluster("cuisine_Thalis")

print(df.columns)

df['name'].head(20)

recommend_by_cluster("KFC")

df['name'].unique().tolist()

def recommend_restaurant(restaurant_name, top_n=5):
    if restaurant_name not in df['name'].values:
        return f"'{restaurant_name}' not found in dataset."

    # index of selected restaurant
    idx = df.index[df['name'] == restaurant_name][0]

    # similarity scores
    scores = list(enumerate(similarity_matrix[idx]))

    # sort high ‚Üí low
    scores = sorted(scores, key=lambda x: x[1], reverse=True)

    results = []
    for i, score in scores[1:top_n+1]:  # skip itself
        results.append({
            "name": df.loc[i, "name"],
            "address": df.loc[i, "address"],
            "city": df.loc[i, "city"],
            "similarity": round(score, 4)
        })

    return results

recommend_restaurant("KFC", top_n=5)

recommend_restaurant("Ludos Pizza", top_n=5)

swiggy_df.info()

swiggy_df.to_csv("swiggy_file.csv", index = False)

# encode_and_save.py
import pandas as pd
import numpy as np
import pickle
from sklearn.preprocessing import OneHotEncoder


df = pd.read_csv("/content/swiggy_file.csv")

# Ensure required columns present
required = {'name','city','cuisine','rating','rating_count','cost','address'}
missing = required - set(df.columns)
if missing:
    raise SystemExit(f"Missing columns: {missing}")

# Keep the original cleaned data intact (we will use this for mapping later)
cleaned = df.copy()

# Columns to encode
cat_cols = ['city','cuisine']
num_cols = ['rating','rating_count','cost']

# Clean numeric columns: coerce, fill with median
for c in num_cols:
    df[c] = pd.to_numeric(df[c], errors='coerce')
    df[c] = df[c].fillna(df[c].median() if not df[c].isna().all() else 0)

# Fill categorical NaNs
df[cat_cols] = df[cat_cols].fillna("unknown").astype(str)

# Fit OneHotEncoder (compatibility across sklearn versions)
from sklearn.preprocessing import OneHotEncoder
try:
    ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')
except TypeError:
    ohe = OneHotEncoder(handle_unknown='ignore')

ohe.fit(df[cat_cols])
ohe_arr = ohe.transform(df[cat_cols])
# if sparse matrix, convert
if not isinstance(ohe_arr, (np.ndarray,)):
    try:
        ohe_arr = ohe_arr.toarray()
    except:
        ohe_arr = np.asarray(ohe_arr)

ohe_cols = list(ohe.get_feature_names_out(cat_cols))
encoded_ohe_df = pd.DataFrame(ohe_arr, columns=ohe_cols, index=df.index)

# Final encoded features (numeric + ohe). Exclude name and address from features.
feature_df = pd.concat([df[num_cols].reset_index(drop=True), encoded_ohe_df.reset_index(drop=True)], axis=1)

# Save encoded dataset and encoder
feature_df.to_csv(ENCODED_PATH, index=False)
with open(ENCODER_PATH, "wb") as f:
    pickle.dump(ohe, f)

# Also save a copy of cleaned file (if you want)
cleaned.to_csv("/mnt/data/cleaned_data_saved.csv", index=False)

print("Saved:")
print(" - encoded features:", ENCODED_PATH)
print(" - encoder:", ENCODER_PATH)
print(" - cleaned copy:", "/mnt/data/cleaned_data_saved.csv")

import pandas as pd
import re

df = pd.read_csv("/content/swiggy_file.csv")

# Clean rating
df["rating"] = pd.to_numeric(df["rating"], errors="coerce")

# Clean rating_count (remove commas)
df["rating_count"] = df["rating_count"].astype(str).str.replace(",", "")
df["rating_count"] = pd.to_numeric(df["rating_count"], errors="coerce")

# Clean cost (‚Çπ250, 250 for two, 250pp ‚Üí 250)
def clean_cost(x):
    x = re.sub(r"[^0-9]", "", str(x))
    return x

df["cost"] = df["cost"].apply(clean_cost)
df["cost"] = pd.to_numeric(df["cost"], errors="coerce")

print(df.dtypes)

swiggy_df.info()

from sklearn.preprocessing import OneHotEncoder

cat_cols = ["city", "cuisine"]

ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
ohe_array = ohe.fit_transform(swiggy_df[cat_cols])

ohe_df = pd.DataFrame(
    ohe_array,
    columns=ohe.get_feature_names_out(cat_cols)
).reset_index(drop=True)

swiggy_encoded = pd.concat([swiggy_df.reset_index(drop=True), ohe_df], axis=1)

numeric_df = df[["rating", "rating_count", "cost"]].reset_index(drop=True)

encoded_final = pd.concat([numeric_df, ohe_df], axis=1)
encoded_final.to_csv("encoded_data.csv", index=False)

import pickle

with open("encoder.pkl", "wb") as f:
    pickle.dump(ohe, f)

df = swiggy_df.copy()

# Clean rating (object ‚Üí float)
df["rating"] = pd.to_numeric(df["rating"], errors="coerce")

# Clean rating_count (remove commas)
df["rating_count"] = df["rating_count"].str.replace(",", "", regex=False)
df["rating_count"] = pd.to_numeric(df["rating_count"], errors="coerce")

# Clean cost (extract numbers only)
df["cost"] = df["cost"].str.extract(r'(\d+)').astype(float)

from sklearn.preprocessing import OneHotEncoder

cat_cols = ["city", "cuisine"]

ohe = OneHotEncoder(sparse_output=False, handle_unknown="ignore")
ohe_array = ohe.fit_transform(df[cat_cols])

ohe_df = pd.DataFrame(
    ohe_array,
    columns=ohe.get_feature_names_out(cat_cols)
)

df = pd.concat([df.reset_index(drop=True), ohe_df], axis=1)
df.drop(columns=cat_cols, inplace=True)

df.isna().sum()

df = df.dropna()

df.isna().sum()

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

features = df.select_dtypes(include=['float', 'int'])  # only numeric columns

scaler = StandardScaler()
scaled_data = scaler.fit_transform(features)

kmeans = KMeans(n_clusters=6, random_state=42)
clusters = kmeans.fit_predict(scaled_data)

df["cluster"] = clusters

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity

scaler = StandardScaler()
scaled_data = scaler.fit_transform(encoded_final)

kmeans = KMeans(n_clusters=6, random_state=42)
clusters = kmeans.fit_predict(scaled_data)

df["cluster"] = clusters  # attach cluster to original data
df.to_csv("cleaned_with_cluster.csv", index=False)

# Cosine Similarity
cos_sim = cosine_similarity(scaled_data)

def recommend_restaurant(name, top_n=5):
    if name not in df["name"].values:
        return "Restaurant not found"

    idx = df.index[df["name"] == name][0]
    scores = list(enumerate(cos_sim[idx]))
    scores = sorted(scores, key=lambda x: x[1], reverse=True)

    top_matches = scores[1:top_n+1]

    results = []
    for i, score in top_matches:
        results.append({
            "name": df.loc[i, "name"],
            "city": df.loc[i, "city"],
            "cuisine": df.loc[i, "cuisine"],
            "similarity": round(score, 4)
        })
    return results

import pandas as pd

df = pd.read_csv("swiggy_file.csv")
print(df.head())
print(df.isna().sum())

swiggy_df["rating"] = pd.to_numeric(swiggy_df["rating"], errors="coerce")
swiggy_df["rating_count"] = pd.to_numeric(swiggy_df["rating_count"].str.replace(',', ''), errors="coerce")
swiggy_df["cost"] = pd.to_numeric(swiggy_df["cost"], errors="coerce")

swiggy_df.info()

from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse_output=False, handle_unknown="ignore")
ohe_cols = ["city", "cuisine"]

ohe_data = ohe.fit_transform(swiggy_df[ohe_cols])

ohe_df = pd.DataFrame(ohe_data, columns=ohe.get_feature_names_out(ohe_cols))

df_encoded = pd.concat([swiggy_df.drop(columns=ohe_cols).reset_index(drop=True),
                        ohe_df.reset_index(drop=True)], axis=1)

swiggy_df.info()

df_encoded

import pandas as pd
from sklearn.preprocessing import OneHotEncoder, LabelEncoder

# Load your dataset
df = pd.read_csv("/content/swiggy_file.csv")

# Step 1: One-Hot Encode city & cuisine
ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')

# Fit and transform
ohe_features = ohe.fit_transform(df[['city', 'cuisine']])

# Convert to DataFrame with proper column names
ohe_df = pd.DataFrame(ohe_features, columns=ohe.get_feature_names_out(['city', 'cuisine']))

# Step 2: Label Encode other categorical columns
label_cols = ['name', 'rating', 'rating_count', 'cost', 'address']
le_dict = {}  # store encoders if you need inverse transform later

for col in label_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    le_dict[col] = le  # save encoder for future use

# Step 3: Concatenate everything
final_df = pd.concat([df[label_cols], ohe_df], axis=1)

# Now final_df contains encoded features
print(final_df.head())

final_df.head()

X = final_df.values  # numpy array of features

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=8, random_state=42)  # choose k based on elbow method
final_df['cluster'] = kmeans.fit_predict(X)

from sklearn.metrics.pairwise import cosine_similarity

similarity_matrix = cosine_similarity(X)

def recommend(index, top_n=5):
    sims = similarity_matrix[index]
    top_indices = sims.argsort()[::-1][1:top_n+1]  # skip self

    # Get the recommended rows from the original (but modified by label encoding) df
    recommended_df = df.iloc[top_indices].copy()

    # Decode label-encoded columns back to original strings for readability
    if 'name' in le_dict:
        recommended_df['name'] = le_dict['name'].inverse_transform(recommended_df['name'].astype(int))
    if 'rating' in le_dict:
        # Convert to int, as LabelEncoder expects integer labels
        recommended_df['rating'] = le_dict['rating'].inverse_transform(recommended_df['rating'].astype(int))
    if 'rating_count' in le_dict:
        recommended_df['rating_count'] = le_dict['rating_count'].inverse_transform(recommended_df['rating_count'].astype(int))
    if 'cost' in le_dict:
        recommended_df['cost'] = le_dict['cost'].inverse_transform(recommended_df['cost'].astype(int))
    if 'address' in le_dict:
        recommended_df['address'] = le_dict['address'].inverse_transform(recommended_df['address'].astype(int))

    return recommended_df

recommendations = recommend(index=42, top_n=5)
print(recommendations[['name','city','cuisine','rating']])

user_choice = "Paradise Biryani"

try:
    # Transform the user's choice (string) to its label-encoded integer value
    user_choice_label_encoded = le_dict['name'].transform([user_choice])[0]
    # Find the index of the restaurant using its label-encoded name in the df
    idx = df[df['name'] == user_choice_label_encoded].index[0]
    recommendations = recommend(idx, top_n=5)
    print(f"Recommendations for '{user_choice}':")
    print(recommendations[['name', 'city', 'cuisine', 'rating']])
except ValueError:
    print(f"Restaurant '{user_choice}' not found in the dataset (could not be encoded).")
except IndexError:
    print(f"Restaurant '{user_choice}' not found in the dataset.")

user_city = "Bangalore"
user_cuisine = "South Indian"
filtered = df[(df['city'] == user_city) &
                       (df['cuisine'] == user_cuisine)]

user_city = "Bangalore"
user_cuisine = "South Indian"

filtered = df[
    (df['city'].str.contains(user_city, case=False, na=False)) &
    (df['cuisine'].str.contains(user_cuisine, case=False, na=False))
]

print(filtered[['name','city','cuisine','cost','rating']].head())

import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

# Assume you already have:
# final_df = encoded numeric features
# original_df = human-readable details

# Step 1: User preferences
user_city = "Bangalore"
user_cuisine = "South Indian"
max_cost = 300

filtered = df[
    (df['city'].str.contains(user_city, case=False, na=False)) &
    (df['cuisine'].str.contains(user_cuisine, case=False, na=False)) &
    (pd.to_numeric(df['cost'], errors='coerce') <= max_cost)
]

# Step 2: Get indices of filtered rows
filtered_indices = filtered.index

# Step 3: Cosine similarity within filtered set
X = final_df.values
filtered_vectors = X[filtered_indices]

# Example: user picks one restaurant from filtered set
chosen_idx = filtered_indices[0]   # first match
chosen_vector = X[chosen_idx].reshape(1, -1)

sims = cosine_similarity(chosen_vector, filtered_vectors)[0]

# Step 4: Rank top recommendations
top_indices = sims.argsort()[::-1][1:6]  # skip self
recommended = filtered.iloc[top_indices]

print(recommended[['name','city','cuisine','cost','rating']])

# If your filtered DataFrame is called `filtered`
names = filtered['name'].tolist()

print(names)

filtered['name'].iloc[0]

for n in filtered['name']:
    print(n)

filtered[['name','city']]

# original_df still has the real names
print(swiggy_df['name'].head())

df.head()

swiggy_df.head()

df.head()

final_df.head()

import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

# Step 1: Build similarity matrix from encoded features
X = final_df.values
similarity_matrix = cosine_similarity(X)

# Step 2: Recommendation function by restaurant name
def recommend_by_name(user_name, top_n=5):
    # Find index of the restaurant in swiggy_df
    try:
        idx = swiggy_df[swiggy_df['name'] == user_name].index[0]
    except IndexError:
        return f"Restaurant '{user_name}' not found in dataset."

    # Get similarity scores
    sims = similarity_matrix[idx]

    # Rank top recommendations (skip the restaurant itself)
    top_indices = sims.argsort()[::-1][1:top_n+1]

    # Return human-readable details
    return swiggy_df.iloc[top_indices][['name','city','cuisine','cost','rating']]

# Example usage
print(recommend_by_name("Paradise Biryani", top_n=5))

# Convert cost to numeric, replace NaN with 0 or median
swiggy_df['cost'] = pd.to_numeric(swiggy_df['cost'], errors='coerce')
swiggy_df['cost'] = swiggy_df['cost'].fillna(0)  # or swiggy_df['cost'].fillna(swiggy_df['cost'].median())

from sklearn.metrics.pairwise import cosine_similarity

X = final_df.values
similarity_matrix = cosine_similarity(X)

def recommend_by_name(user_name, top_n=5):
    try:
        idx = swiggy_df[swiggy_df['name'] == user_name].index[0]
    except IndexError:
        return f"Restaurant '{user_name}' not found."

    sims = similarity_matrix[idx]
    top_indices = sims.argsort()[::-1][1:top_n+1]

    return swiggy_df.iloc[top_indices][['name','city','cuisine','cost','rating']]

# Example usage
print(recommend_by_name("Paradise Biryani", top_n=5))

# Convert cost to numeric, but keep missing values as NaN
swiggy_df['cost'] = pd.to_numeric(swiggy_df['cost'], errors='coerce')

# Replace NaN with median or mean cost instead of 0
swiggy_df['cost'] = swiggy_df['cost'].fillna(swiggy_df['cost'].median())

from sklearn.metrics.pairwise import cosine_similarity

X = final_df.values
similarity_matrix = cosine_similarity(X)

def recommend_by_name(user_name, top_n=5):
    try:
        idx = swiggy_df[swiggy_df['name'] == user_name].index[0]
    except IndexError:
        return f"Restaurant '{user_name}' not found."

    sims = similarity_matrix[idx]
    top_indices = sims.argsort()[::-1][1:top_n+1]

    return swiggy_df.iloc[top_indices][['name','city','cuisine','cost','rating']]

# Example usage
print(recommend_by_name("Paradise Biryani", top_n=5))

decoded_names = le_dict['name'].inverse_transform(final_df['name'])
print(decoded_names[:10])

# swiggy_df = human-readable data (keep 'name' as string)
# final_df = encoded features (exclude 'name')

X = final_df.values
similarity_matrix = cosine_similarity(X)

def recommend_by_name(user_name, top_n=5):
    # Find index of the restaurant in swiggy_df
    try:
        idx = swiggy_df[swiggy_df['name'] == user_name].index[0]
    except IndexError:
        return f"Restaurant '{user_name}' not found."

    # Get similarity scores
    sims = similarity_matrix[idx]
    top_indices = sims.argsort()[::-1][1:top_n+1]

    # Return human-readable details
    return swiggy_df.iloc[top_indices][['name','city','cuisine','cost','rating']]

print(recommend_by_name("Paradise Biryani", top_n=5))

def recommend_by_name(user_name, top_n=5):
    # Find index of the restaurant in swiggy_df (case-insensitive, partial match)
    matches = swiggy_df[swiggy_df['name'].str.contains(user_name, case=False, na=False)]

    if matches.empty:
        return f"Restaurant '{user_name}' not found."

    # Take the first match (or you can loop through matches)
    idx = matches.index[0]

    # Get similarity scores
    sims = similarity_matrix[idx]
    top_indices = sims.argsort()[::-1][1:top_n+1]

    # Return human-readable details
    return swiggy_df.iloc[top_indices][['name','city','cuisine','cost','rating']]

print(recommend_by_name("Paradise Biryani", top_n=5))

from sklearn.metrics.pairwise import cosine_similarity

X = final_df.values
similarity_matrix = cosine_similarity(X)

def recommend_by_name(user_name, top_n=5):
    # Find all matches (case-insensitive, partial match)
    matches = swiggy_df[swiggy_df['name'].str.contains(user_name, case=False, na=False)]

    if matches.empty:
        return f"Restaurant '{user_name}' not found."

    # If multiple matches, show them to the user
    if len(matches) > 1:
        print("Multiple matches found. Please choose one:")
        for i, row in matches.iterrows():
            print(f"Index: {i}, Name: {row['name']}, City: {row['city']}, Cuisine: {row['cuisine']}")
        return "Please provide the index of the restaurant you want."

    # If only one match, proceed directly
    idx = matches.index[0]
    sims = similarity_matrix[idx]
    top_indices = sims.argsort()[::-1][1:top_n+1]

    return swiggy_df.iloc[top_indices][['name','city','cuisine','cost','rating']]

# User asks for Paradise Biryani
result = recommend_by_name("Paradise Biryani", top_n=5)
print(result)

chosen_idx = 9304  # user selects Kolkata branch
sims = similarity_matrix[chosen_idx]
top_indices = sims.argsort()[::-1][1:6]
print(swiggy_df.iloc[top_indices][['name','city','cuisine','cost','rating']])

# Remove currency symbols and text
swiggy_df['cost'] = (
    swiggy_df['cost']
    .astype(str)                # ensure string
    .str.replace(r'[^0-9]', '', regex=True)  # keep only digits
)

# Convert to numeric
swiggy_df['cost'] = pd.to_numeric(swiggy_df['cost'], errors='coerce')

# Fill missing values with median or mean instead of 0
swiggy_df['cost'] = swiggy_df['cost'].fillna(swiggy_df['cost'].median())

print(swiggy_df[['name','cost']].head(10))

"""# #Swiggy Recommendation Workings 25.11.2025"""

import pandas as pd
import numpy as np

swiggy = pd.read_csv("/content/swiggy.csv")

swiggy.info()

# Taking 12,000 data as sample for workings


sample = swiggy.sample(n=12000, random_state=42)   # select 12,000 rows

sample.to_csv("swiggy_sample.csv", index=False)
print("Sample file created!")

swiggy_df = pd.read_csv("/content/swiggy_sample.csv")

swiggy_df.info()

swiggy_df.head()

swiggy_df.isna().sum()

# Filling null value with "bfill" & "ffill" methods
swiggy_df['name'] = swiggy_df['name'].fillna(method = "bfill")
swiggy_df["rating"] = swiggy_df["rating"].fillna(method = "bfill")
swiggy_df["rating_count"] = swiggy_df["rating_count"].fillna(method = "bfill")
swiggy_df["cost"] = swiggy_df["cost"].fillna(method = "ffill")
swiggy_df["address"] = swiggy_df["address"].fillna(method = "ffill")
swiggy_df["cuisine"] = swiggy_df["cuisine"].fillna(method = "ffill")
swiggy_df["lic_no"] = swiggy_df["lic_no"].fillna(method = "ffill")

swiggy_df.isnull().sum()

swiggy_df.to_csv("swiggy_cleaned.csv", index=False)

swiggy_df.drop_duplicates(inplace=True)

swiggy_df.info()

import pandas as pd
import numpy as np

swiggy_cleaned = pd.read_csv("/content/swiggy_cleaned_12000.csv")

swiggy_cleaned.info()

# Drop multiple columns from the DataFrame
swiggy_cleaned = swiggy_cleaned.drop(columns=["id", "lic_no", "link", "menu"])
print(swiggy_cleaned.head())

swiggy_cleaned.info()

import pandas as pd
import pickle

# Assume swiggy_df is your DataFrame
# One-Hot Encode the 'city' column
city_ohe = pd.get_dummies(swiggy_cleaned['city'], prefix='city')

# Save to pickle file
with open("city_ohe.pkl", "wb") as f:
    pickle.dump(city_ohe, f)

print("City OHE saved as city_ohe.pkl")

import pandas as pd
import pickle

# One-Hot Encode the 'cuisine' column
cuisine_ohe = pd.get_dummies(swiggy_cleaned['cuisine'], prefix='cuisine')

# Save to pickle file
with open("cuisine_ohe.pkl", "wb") as f:
    pickle.dump(cuisine_ohe, f)

print("Cuisine OHE saved as cuisine_ohe.pkl")

# Remove rupee symbol and commas, then convert to numeric
swiggy_cleaned['cost'] = (
    swiggy_cleaned['cost']
    .str.replace('‚Çπ', '', regex=False)   # remove rupee symbol
    .str.replace(',', '', regex=False)   # remove commas in numbers
    .astype(float)                       # convert to float
)

print(swiggy_cleaned['cost'].head())

swiggy_cleaned.head()

swiggy_cleaned['rating'].unique()

swiggy_cleaned['rating'] = (
    swiggy_cleaned['rating']
        .str.replace('--', '', regex=False)  # Remove '--'
        .str.replace(',', '', regex=False)   # remove commas
)
swiggy_cleaned['rating'] = pd.to_numeric(swiggy_cleaned['rating'], errors='coerce')

swiggy_cleaned['rating'].unique()

swiggy_cleaned.head()

# Replace NaN with 0 in the 'rating' column
swiggy_cleaned['rating'] = swiggy_cleaned['rating'].fillna(0)

swiggy_cleaned.head()

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OrdinalEncoder

# Initialize encoder
le = LabelEncoder()
oe = OrdinalEncoder()

# Encode 'name' column
swiggy_cleaned['name'] = le.fit_transform(swiggy_cleaned['name'].astype(str))

# Encode 'address' column
swiggy_cleaned['address'] = le.fit_transform(swiggy_cleaned['address'].astype(str))

# Encode 'rating_count' column, converting Series to DataFrame for OrdinalEncoder
swiggy_cleaned['rating_count'] = oe.fit_transform(swiggy_cleaned[['rating_count']].astype(str))

print(swiggy_cleaned[['name','address','rating_count']].head())

swiggy_cleaned.head()

import pickle
import pandas as pd

# 1. Load the city OHE pickle
with open("city_ohe.pkl", "rb") as f:
    city_ohe = pickle.load(f)

# 2. Load the cuisine OHE pickle
with open("cuisine_ohe.pkl", "rb") as f:
    cuisine_ohe = pickle.load(f)

# 3. Merge them with swiggy_cleaned
# Concatenate along columns (axis=1)
swiggy_cleaned = pd.concat([swiggy_cleaned, city_ohe, cuisine_ohe], axis=1)

print(swiggy_cleaned.head())

swiggy_cleaned.head()

# Drop the original city and cuisine columns
swiggy_cleaned = swiggy_cleaned.drop(columns=['city', 'cuisine'])

print(swiggy_cleaned.head())

swiggy_cleaned.info()

swiggy_cleaned.to_csv("swiggy_encoded", index=False)

import pandas as pd
import pickle

swiggy_encoded = pd.read_csv("/content/swiggy_encoded")

swiggy_encoded.head()

from sklearn.cluster import KMeans

# Choose number of clusters
kmeans = KMeans(n_clusters=5, random_state=42)

# Fit the model
swiggy_encoded['cluster'] = kmeans.fit_predict(swiggy_encoded)

print(swiggy_encoded[['cluster']].head())

from sklearn.cluster import KMeans

# Choose number of clusters
kmeans = KMeans(n_clusters=10, random_state=42)

# Fit the model
swiggy_encoded['cluster'] = kmeans.fit_predict(swiggy_encoded)

print(swiggy_encoded[['cluster']].head())

from sklearn.metrics.pairwise import cosine_similarity

# Compute cosine similarity matrix
cosine_sim = cosine_similarity(swiggy_encoded.drop(columns=['cluster']))

# Example: get similarity scores for the first restaurant
similarities = list(enumerate(cosine_sim[0]))

# Sort by similarity (excluding itself at index 0)
similarities = sorted(similarities, key=lambda x: x[1], reverse=True)

print(similarities[:5])  # top 5 most similar items

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity

# 1. Load both files
swiggy_cleaned = pd.read_csv("/content/swiggy_cleaned (1).csv")   # raw file with names, addresses, etc.
swiggy_encoded = pd.read_csv("/content/swiggy_encoded")   # numeric encoded file

# 2. Apply K-Means clustering
kmeans = KMeans(n_clusters=5, random_state=42)
swiggy_encoded['cluster'] = kmeans.fit_predict(swiggy_encoded)

# 3. Compute cosine similarity (drop cluster column for similarity calculation)
cosine_sim = cosine_similarity(swiggy_encoded.drop(columns=['cluster']))

# 4. Function to get similar restaurants
def get_similar_restaurants(index, top_n=5):
    # Get similarity scores for the given restaurant index
    sim_scores = list(enumerate(cosine_sim[index]))
    # Sort by similarity (highest first, skip itself at index)
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    top_indices = [i[0] for i in sim_scores[1:top_n+1]]
    # Map back to original file for human-readable output
    return swiggy_cleaned.iloc[top_indices][['name','address','cost','rating','cuisine']]

# 5. Example usage: pick restaurant at index 10
print("Cluster assignment:", swiggy_encoded.loc[10, 'cluster'])
print("Similar restaurants to index 10:")
print(get_similar_restaurants(10, top_n=5))

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity

# 1. Load both files
swiggy_cleaned = pd.read_csv("/content/swiggy_cleaned (1).csv")   # raw file with names, addresses, etc.
swiggy_encoded = pd.read_csv("/content/swiggy_encoded")   # numeric encoded file

# 2. Apply K-Means clustering
kmeans = KMeans(n_clusters=5, random_state=42)
swiggy_encoded['cluster'] = kmeans.fit_predict(swiggy_encoded)

# 3. Compute cosine similarity (drop cluster column for similarity calculation)
cosine_sim = cosine_similarity(swiggy_encoded.drop(columns=['cluster']))

# 4. Build a mapping from restaurant name to index
indices = pd.Series(swiggy_cleaned.index, index=swiggy_cleaned['name']).drop_duplicates()

# 5. Function to get similar restaurants by name
def get_similar_restaurants_by_name(name, top_n=5):
    if name not in indices:
        return f"Restaurant '{name}' not found in dataset."
    idx = indices[name]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    top_indices = [i[0] for i in sim_scores[1:top_n+1]]
    return swiggy_cleaned.iloc[top_indices][['name','address','cost','rating','cuisine']]

# 6. Example usage
# Corrected: Using an actual restaurant name 'The China Town' instead of a cuisine type 'Chinese,Indian'
# Also, accessing 'cluster' from swiggy_encoded which contains the cluster assignments
print("Cluster assignment for The China Town:", swiggy_encoded.loc[indices['The China Town'], 'cluster'])
print("Similar restaurants to The China Town:")
print(get_similar_restaurants_by_name("The China Town", top_n=5))

"""## Smart Premium workings #"""

import pandas as pd
import numpy as np

train = pd.read_csv("/content/train.csv")

train.info()

train.head()

# Taking 2,00,000 data as sample for workings


sample = train.sample(n=200000, random_state=42)   # select 200000 rows

sample.to_csv("train_sample.csv", index=False)
print("Sample file created!")

train_sample = pd.read_csv("/content/train_sample.csv")

train_sample.head()

train_sample.info()

for i in train_sample.columns:
     if train_sample[i].isna().sum():
        print(i)

for i in train_sample.columns:
    print(i, train_sample[i].nunique())

for i in train_sample.columns:
    if train_sample[i].dtypes == 'object':
       print(i)

# Calculate the mode of the column
marital_mode = train_sample['Marital Status'].mode()[0]
gender_mode = train_sample['Gender'].mode()[0]
education_mode = train_sample['Education Level'].mode()[0]
occupation_mode = train_sample['Occupation'].mode()[0]
location_mode = train_sample['Location'].mode()[0]
policy_mode = train_sample['Policy Type'].mode()[0]
policy_date_mode = train_sample['Policy Start Date'].mode()[0]
feedback_mode = train_sample['Customer Feedback'].mode()[0]
smoking_mode = train_sample['Smoking Status'].mode()[0]
exercise_mode = train_sample['Exercise Frequency'].mode()[0]
property_mode = train_sample['Property Type'].mode()[0]


# Fill NaN values with the mode
train_sample['Marital Status'] = train_sample['Marital Status'].fillna(marital_mode)
train_sample['Gender'] = train_sample['Gender'].fillna(gender_mode)
train_sample['Education Level'] = train_sample['Education Level'].fillna(education_mode)
train_sample['Occupation'] = train_sample['Occupation'].fillna(occupation_mode)
train_sample['Location'] = train_sample['Location'].fillna(location_mode)
train_sample['Policy Type'] = train_sample['Policy Type'].fillna(policy_mode)
train_sample['Policy Start Date'] = train_sample['Policy Start Date'].fillna(policy_date_mode)
train_sample['Customer Feedback'] = train_sample['Customer Feedback'].fillna(feedback_mode)
train_sample['Smoking Status'] = train_sample['Smoking Status'].fillna(smoking_mode)
train_sample['Exercise Frequency'] = train_sample['Exercise Frequency'].fillna(exercise_mode)
train_sample['Property Type'] = train_sample['Property Type'].fillna(property_mode)

for i in train_sample.columns:
     if train_sample[i].isna().sum():
        print(i)

# Calculate the median of the column
age_median = train_sample['Age'].median()
income_median = train_sample['Annual Income'].median()
dependent_median = train_sample['Number of Dependents'].median()
health_median = train_sample['Health Score'].median()
claim_median = train_sample['Previous Claims'].median()
vehicle_median = train_sample['Vehicle Age'].median()
score_median = train_sample['Credit Score'].median()


# Fill NaN values with the median
train_sample['Age'] = train_sample['Age'].fillna(age_median)
train_sample['Annual Income'] = train_sample['Annual Income'].fillna(income_median)
train_sample['Number of Dependents'] = train_sample['Number of Dependents'].fillna(dependent_median)
train_sample['Health Score'] = train_sample['Health Score'].fillna(health_median)
train_sample['Previous Claims'] = train_sample['Previous Claims'].fillna(claim_median)
train_sample['Vehicle Age'] = train_sample['Vehicle Age'].fillna(vehicle_median)
train_sample['Credit Score'] = train_sample['Credit Score'].fillna(score_median)

for i in train_sample.columns:
     if train_sample[i].isna().sum():
        print(i)

train_sample.isnull().sum()

train_sample.info()

train_sample.to_csv("train_cleaned.csv", index=False)

import pandas as pd
import numpy as np

train_cleaned = pd.read_csv("/content/train_cleaned.csv")

train_cleaned.info()

train_cleaned.head()

for i in train_cleaned.columns:
    if train_cleaned[i].dtypes == 'object':
       print(i)

train_cleaned['Marital Status'].unique()

train_cleaned['Gender'].unique()

train_cleaned['Exercise Frequency'].unique()

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OrdinalEncoder

LE = LabelEncoder()
OE = OrdinalEncoder()

train_cleaned['Gender'] = LE.fit_transform(train_cleaned['Gender'])
train_cleaned['Marital Status'] = LE.fit_transform(train_cleaned['Marital Status'])
train_cleaned['Smoking Status'] = LE.fit_transform(train_cleaned['Smoking Status'])

# Fit and transform the column
train_cleaned['Education Level'] = OE.fit_transform(train_cleaned[['Education Level']])
train_cleaned['Occupation'] = OE.fit_transform(train_cleaned[['Occupation']])
train_cleaned['Location'] = LE.fit_transform(train_cleaned['Location'])
train_cleaned['Policy Type'] = OE.fit_transform(train_cleaned[['Policy Type']])
train_cleaned['Customer Feedback'] = OE.fit_transform(train_cleaned[['Customer Feedback']])
train_cleaned['Exercise Frequency'] = LE.fit_transform(train_cleaned['Exercise Frequency'])
train_cleaned['Property Type'] = LE.fit_transform(train_cleaned['Property Type'])

train_cleaned.info()

train_cleaned.head()

train_cleaned['Policy Start Date'] = pd.to_datetime(train_cleaned['Policy Start Date'])

train_cleaned['Policy date'] = train_cleaned['Policy Start Date'].dt.day
train_cleaned['Policy Month'] = train_cleaned['Policy Start Date'].dt.month
train_cleaned['Policy Year'] = train_cleaned['Policy Start Date'].dt.year

train_cleaned.head()

train_cleaned = train_cleaned.drop(columns=['Policy Start Date'], axis=1, errors='ignore')

train_cleaned.to_csv("train_cleaned_ML.csv", index=False)

import pandas as pd

train_cleaned = pd.read_csv("/content/train_cleaned_ML.csv")

train_cleaned = train_cleaned.drop(columns = ['id'], axis = 1)

train_cleaned.head()

train_cleaned.skew()

train_cleaned.kurtosis()

train_cleaned.corr()

train_cleaned.corr()["Premium Amount"].sort_values(ascending=False)

from sklearn.preprocessing import StandardScaler
scalar = StandardScaler()

X= train_cleaned.drop(columns = ["Premium Amount"], axis = 1)
y = train_cleaned["Premium Amount"]

X

y

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaled_train = scalar.fit_transform(X_train)

scaled_test = scalar.transform(X_test)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

model = RandomForestRegressor(n_estimators=100, max_depth = 10, random_state=42)
model.fit(scaled_train, y_train)
pred = model.predict(scaled_test)
mae = mean_absolute_error(y_test, pred)
mse = mean_squared_error(y_test, pred)
r2 = r2_score(y_test, pred)

# Print results
print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("R¬≤ Score:", r2)

from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(scaled_train, y_train)
pred = model.predict(scaled_test)
mae = mean_absolute_error(y_test, pred)
mse = mean_squared_error(y_test, pred)
r2 = r2_score(y_test, pred)
# Print results
print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("R¬≤ Score:", r2)

from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
model = RandomForestRegressor(n_estimators=300, max_depth = 10, random_state=42)
model.fit(scaled_train, y_train)
pred = model.predict(scaled_test)
mae = mean_absolute_error(y_test, pred)
mse = mean_squared_error(y_test, pred)
r2 = r2_score(y_test, pred)
# Print results
print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("R¬≤ Score:", r2)

from sklearn.tree import DecisionTreeRegressor
model = DecisionTreeRegressor(max_depth = 10, random_state=42)
model.fit(scaled_train, y_train)
pred = model.predict(scaled_test)
mae = mean_absolute_error(y_test, pred)
mse = mean_squared_error(y_test, pred)
r2 = r2_score(y_test, pred)
# Print results
print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("R¬≤ Score:", r2)

from sklearn.ensemble import GradientBoostingRegressor
model = GradientBoostingRegressor()
model.fit(scaled_train, y_train)
pred = model.predict(scaled_test)
mae = mean_absolute_error(y_test, pred)
mse = mean_squared_error(y_test, pred)
r2 = r2_score(y_test, pred)
# Print results
print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("R¬≤ Score:", r2)